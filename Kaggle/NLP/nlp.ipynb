{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "85c7676fa209e24fc71b0c5e47e3f31bd7531426112dde9960a8a46a33b066c3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# train set\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# test set\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nCountVectorizer는 다음과 같은 세가지 작업을 수행한다.\\n\\n1. 문서를 토큰 리스트로 변환한다.\\n\\n2. 각 문서에서 토큰의 출현 빈도를 센다.\\n\\n3. 각 문서를 BOW 인코딩 벡터로 변환한다.\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "'''\n",
    "Scikit-Learn의 feature_extraction 서브패키지와 feature_extraction.text 서브패키지는 다음과 같은 문서 전처리용 클래스를 제공한다.\n",
    "\n",
    "# DictVectorizer:\n",
    "    각 단어의 수를 세어놓은 사전에서 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "# CountVectorizer: \n",
    "    문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "# TfidfVectorizer:\n",
    "    CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BOW 인코딩 벡터를 만든다.\n",
    "\n",
    "# HashingVectorizer:\n",
    "    해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BOW 인코딩 벡터를 만든다.\n",
    "'''\n",
    "\n",
    "'''\n",
    "CountVectorizer는 다음과 같은 세가지 작업을 수행한다.\n",
    "\n",
    "1. 문서를 토큰 리스트로 변환한다.\n",
    "\n",
    "2. 각 문서에서 토큰의 출현 빈도를 센다.\n",
    "\n",
    "3. 각 문서를 BOW 인코딩 벡터로 변환한다.\n",
    "'''\n",
    "# https://datascienceschool.net/03%20machine%20learning/03.01.03%20Scikit-Learn%EC%9D%98%20%EB%AC%B8%EC%84%9C%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set의 text fit_transform()\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set의 text transform()\n",
    "# train set의 평균, 표준편차를 동일하게 사용하기 위해 test set에는 fit_transform()이 아닌 transform()사용\n",
    "test_vectors = count_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.70133964, 0.6501182 , 0.71147024])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# disaster을 나타내는 문장이 맞는지 아닌지 선형 분류\n",
    "# 과대적합을 피하기 위해서 Ridge사용\n",
    "clf_Ridge = linear_model.RidgeClassifier(random_state=42)\n",
    "clf_Ridge.fit(train_vectors, train_df['target'])\n",
    "score_Ridge = model_selection.cross_val_score(clf_Ridge, train_vectors, train_df['target'], cv=3, scoring='accuracy')\n",
    "score_Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.71158392, 0.67178881, 0.70871108])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# SGDClassifier 사용\n",
    "clf_SGD = linear_model.SGDClassifier(random_state=42)\n",
    "clf_SGD.fit(train_vectors, train_df['target'])\n",
    "score_SGD = model_selection.cross_val_score(clf_SGD, train_vectors, train_df['target'], cv=3, scoring='accuracy')\n",
    "score_SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "answer = pd.read_csv(\"sample_submission.csv\")\n",
    "answer_target = clf_Ridge.predict(test_vectors)\n",
    "print(answer_target)\n",
    "answer['target'] = answer_target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.to_csv(\"mymy.csv\", index=False)"
   ]
  }
 ]
}